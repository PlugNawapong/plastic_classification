{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plastic Classification with PCA Band Reduction\n",
    "## Training on Google Colab Pro+\n",
    "\n",
    "This notebook trains a 1D CNN classifier on PCA-reduced hyperspectral bands.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load normalized hyperspectral data (459 bands)\n",
    "2. Apply PCA to reduce dimensions (e.g., 459 → 150 bands)\n",
    "3. Train pixel-wise 1D CNN classifier\n",
    "4. Compare multiple PCA configurations\n",
    "\n",
    "**Setup Requirements:**\n",
    "- Google Colab Pro+ (for GPU/High-RAM)\n",
    "- Runtime: GPU (T4, V100, or A100)\n",
    "- Upload your dataset to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q scikit-learn pillow tqdm matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Paths\n",
    "\n",
    "**IMPORTANT:** Update these paths to point to your dataset in Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURE THESE PATHS\n",
    "# ==============================================================================\n",
    "\n",
    "# Example: If your dataset is in 'My Drive/plastic_classification/'\n",
    "DRIVE_BASE = '/content/drive/MyDrive/plastic_classification'\n",
    "\n",
    "# Dataset paths\n",
    "TRAIN_DATASET = os.path.join(DRIVE_BASE, 'training_dataset')\n",
    "LABEL_PATH = os.path.join(DRIVE_BASE, 'Ground_Truth/labels.json')\n",
    "\n",
    "# Output paths (results will be saved here)\n",
    "OUTPUT_DIR = os.path.join(DRIVE_BASE, 'colab_results')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset: {TRAIN_DATASET}\")\n",
    "print(f\"Labels: {LABEL_PATH}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA Band Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCABandReducer:\n",
    "    \"\"\"PCA-based dimensionality reduction for hyperspectral data.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=None, variance_threshold=0.99):\n",
    "        self.n_components = n_components\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.pca = None\n",
    "        self.scaler = None\n",
    "        self.n_components_selected = None\n",
    "        \n",
    "    def fit(self, hypercube):\n",
    "        \"\"\"Fit PCA on hypercube.\"\"\"\n",
    "        n_bands, height, width = hypercube.shape\n",
    "        X = hypercube.reshape(n_bands, -1).T  # (n_pixels, n_bands)\n",
    "        \n",
    "        print(f\"Fitting PCA on {X.shape[0]:,} pixels x {X.shape[1]} bands...\")\n",
    "        \n",
    "        # Standardize\n",
    "        self.scaler = StandardScaler()\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Determine n_components\n",
    "        if self.n_components is None:\n",
    "            pca_full = PCA()\n",
    "            pca_full.fit(X)\n",
    "            cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "            self.n_components_selected = np.argmax(cumsum_var >= self.variance_threshold) + 1\n",
    "        else:\n",
    "            self.n_components_selected = self.n_components\n",
    "        \n",
    "        # Fit PCA\n",
    "        self.pca = PCA(n_components=self.n_components_selected)\n",
    "        self.pca.fit(X)\n",
    "        \n",
    "        variance_explained = np.sum(self.pca.explained_variance_ratio_)\n",
    "        \n",
    "        print(f\"✓ PCA fitted: {n_bands} → {self.n_components_selected} components\")\n",
    "        print(f\"✓ Variance explained: {variance_explained*100:.2f}%\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, spectrum):\n",
    "        \"\"\"Transform spectrum (1D array) to PCA space.\"\"\"\n",
    "        spectrum_2d = spectrum.reshape(1, -1)\n",
    "        spectrum_std = self.scaler.transform(spectrum_2d)\n",
    "        spectrum_pca = self.pca.transform(spectrum_std)\n",
    "        return spectrum_pca.flatten()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save PCA model.\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'pca': self.pca,\n",
    "                'scaler': self.scaler,\n",
    "                'n_components_selected': self.n_components_selected,\n",
    "                'variance_threshold': self.variance_threshold\n",
    "            }, f)\n",
    "        print(f\"✓ PCA model saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        \"\"\"Load PCA model.\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        reducer = cls()\n",
    "        reducer.pca = data['pca']\n",
    "        reducer.scaler = data['scaler']\n",
    "        reducer.n_components_selected = data['n_components_selected']\n",
    "        reducer.variance_threshold = data.get('variance_threshold', 0.99)\n",
    "        \n",
    "        print(f\"✓ PCA model loaded: {reducer.n_components_selected} components\")\n",
    "        return reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hypercube(dataset_path):\n",
    "    \"\"\"Load normalized hyperspectral cube.\"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    # Load wavelengths\n",
    "    with open(dataset_path / 'header.json', 'r') as f:\n",
    "        header = json.load(f)\n",
    "    wavelengths = header['wavelength (nm)']\n",
    "    \n",
    "    # Load bands\n",
    "    print(f\"Loading {len(wavelengths)} bands...\")\n",
    "    bands = []\n",
    "    for i in tqdm(range(1, len(wavelengths) + 1), desc='Loading bands'):\n",
    "        img_path = dataset_path / f'ImagesStack{i:03d}.png'\n",
    "        if img_path.exists():\n",
    "            img = np.array(Image.open(img_path).convert('L'), dtype=np.float32) / 255.0\n",
    "            bands.append(img)\n",
    "    \n",
    "    hypercube = np.stack(bands, axis=0)\n",
    "    print(f\"✓ Hypercube loaded: {hypercube.shape}\")\n",
    "    \n",
    "    return hypercube, wavelengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperspectralDataset(Dataset):\n",
    "    \"\"\"Dataset for pixel-wise hyperspectral classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, label_path, pca_reducer=None, max_samples=None):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.pca_reducer = pca_reducer\n",
    "        \n",
    "        # Load wavelengths\n",
    "        with open(self.dataset_path / 'header.json', 'r') as f:\n",
    "            header = json.load(f)\n",
    "        self.wavelengths = header['wavelength (nm)']\n",
    "        self.n_bands = len(self.wavelengths)\n",
    "        \n",
    "        # Load labels\n",
    "        with open(label_path, 'r') as f:\n",
    "            labels_data = json.load(f)\n",
    "        \n",
    "        # Extract samples\n",
    "        self.samples = []\n",
    "        for label_info in labels_data:\n",
    "            class_id = label_info['label']\n",
    "            for coord in label_info['coordinates']:\n",
    "                x, y = coord\n",
    "                self.samples.append({'x': x, 'y': y, 'label': class_id})\n",
    "        \n",
    "        if max_samples:\n",
    "            self.samples = self.samples[:max_samples]\n",
    "        \n",
    "        print(f\"✓ Dataset: {len(self.samples):,} samples\")\n",
    "    \n",
    "    def load_spectrum(self, x, y):\n",
    "        \"\"\"Load spectral signature for pixel (x, y).\"\"\"\n",
    "        spectrum = np.zeros(self.n_bands, dtype=np.float32)\n",
    "        \n",
    "        for i in range(1, self.n_bands + 1):\n",
    "            img_path = self.dataset_path / f'ImagesStack{i:03d}.png'\n",
    "            if img_path.exists():\n",
    "                img = Image.open(img_path).convert('L')\n",
    "                spectrum[i - 1] = np.array(img)[y, x] / 255.0\n",
    "        \n",
    "        return spectrum\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        x, y, label = sample['x'], sample['y'], sample['label']\n",
    "        \n",
    "        # Load spectrum\n",
    "        spectrum = self.load_spectrum(x, y)\n",
    "        \n",
    "        # Apply PCA if available\n",
    "        if self.pca_reducer:\n",
    "            spectrum = self.pca_reducer.transform(spectrum)\n",
    "        \n",
    "        return torch.from_numpy(spectrum).float(), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 1D CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralCNN1D(nn.Module):\n",
    "    \"\"\"1D CNN for pixel-wise hyperspectral classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_bands, n_classes, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_bands = n_bands\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # 1D Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Calculate flattened size\n",
    "        self.flat_size = 256 * (n_bands // 4)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flat_size, 512)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, n_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: (batch, n_bands)\n",
    "        x = x.unsqueeze(1)  # (batch, 1, n_bands)\n",
    "        \n",
    "        # Conv blocks\n",
    "        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, n_epochs, learning_rate, output_dir, device):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=10, T_mult=2, eta_min=learning_rate/10\n",
    "    )\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{n_epochs} [Train]')\n",
    "        for spectra, labels in pbar:\n",
    "            spectra, labels = spectra.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(spectra)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{train_loss/(pbar.n+1):.4f}',\n",
    "                'acc': f'{100.*train_correct/train_total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for spectra, labels in tqdm(val_loader, desc=f'Epoch {epoch}/{n_epochs} [Val]'):\n",
    "                spectra, labels = spectra.to(device), labels.to(device)\n",
    "                outputs = model(spectra)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}% | \"\n",
    "              f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, os.path.join(output_dir, 'best_model.pth'))\n",
    "            print(f\"✓ Best model saved (Val Acc: {val_acc:.2f}%)\")\n",
    "    \n",
    "    return history, best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'n_classes': 11,\n",
    "    'batch_size': 1024,  # Large batch for Colab Pro+ GPU\n",
    "    'n_epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'dropout_rate': 0.5,\n",
    "    'train_split': 0.9,\n",
    "    'num_workers': 2,\n",
    "    'max_samples': None,  # None = use all samples\n",
    "    \n",
    "    # PCA configurations to test\n",
    "    'pca_configs': [None, 50, 100, 150, 200],  # None = no PCA\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hypercube for PCA fitting (once)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING HYPERCUBE FOR PCA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "hypercube, wavelengths = load_hypercube(TRAIN_DATASET)\n",
    "n_original_bands = hypercube.shape[0]\n",
    "\n",
    "print(f\"\\n✓ Loaded hypercube: {hypercube.shape}\")\n",
    "print(f\"✓ Wavelengths: {wavelengths[0]:.1f} - {wavelengths[-1]:.1f} nm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train with Different PCA Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for pca_comp in CONFIG['pca_configs']:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    if pca_comp is None:\n",
    "        print(\"TRAINING WITHOUT PCA (BASELINE)\")\n",
    "        exp_name = \"no_pca\"\n",
    "        n_bands = n_original_bands\n",
    "    else:\n",
    "        print(f\"TRAINING WITH PCA: {pca_comp} COMPONENTS\")\n",
    "        exp_name = f\"pca_{pca_comp}\"\n",
    "        n_bands = pca_comp\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create output directory\n",
    "    exp_output_dir = os.path.join(OUTPUT_DIR, exp_name)\n",
    "    os.makedirs(exp_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Fit PCA if needed\n",
    "    if pca_comp is None:\n",
    "        pca_reducer = None\n",
    "    else:\n",
    "        print(f\"\\nFitting PCA with {pca_comp} components...\")\n",
    "        pca_reducer = PCABandReducer(n_components=pca_comp)\n",
    "        pca_reducer.fit(hypercube)\n",
    "        pca_reducer.save(os.path.join(exp_output_dir, 'pca_model.pkl'))\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    full_dataset = HyperspectralDataset(\n",
    "        TRAIN_DATASET, LABEL_PATH, pca_reducer, CONFIG['max_samples']\n",
    "    )\n",
    "    \n",
    "    # Split train/val\n",
    "    n_samples = len(full_dataset)\n",
    "    n_train = int(n_samples * CONFIG['train_split'])\n",
    "    n_val = n_samples - n_train\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [n_train, n_val]\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=CONFIG['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Train samples: {n_train:,}\")\n",
    "    print(f\"✓ Val samples: {n_val:,}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(f\"\\nCreating model with {n_bands} input bands...\")\n",
    "    model = SpectralCNN1D(\n",
    "        n_bands=n_bands,\n",
    "        n_classes=CONFIG['n_classes'],\n",
    "        dropout_rate=CONFIG['dropout_rate']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"✓ Model created: {model.get_num_params():,} parameters\")\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nTraining for {CONFIG['n_epochs']} epochs...\")\n",
    "    history, best_val_acc = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        CONFIG['n_epochs'], CONFIG['learning_rate'],\n",
    "        exp_output_dir, device\n",
    "    )\n",
    "    \n",
    "    # Save history\n",
    "    with open(os.path.join(exp_output_dir, 'history.json'), 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    \n",
    "    # Record results\n",
    "    results.append({\n",
    "        'config': exp_name,\n",
    "        'n_components': pca_comp,\n",
    "        'n_bands': n_bands,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'output_dir': exp_output_dir\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n✓ Experiment '{exp_name}' completed!\")\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}%\")\n",
    "    print(f\"  Results saved to: {exp_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Configuration':<20} {'Bands':<10} {'Best Val Acc':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for result in results:\n",
    "    config_name = result['config'].replace('_', ' ').title()\n",
    "    print(f\"{config_name:<20} {result['n_bands']:<10} {result['best_val_acc']:>13.2f}%\")\n",
    "\n",
    "# Find best\n",
    "best_result = max(results, key=lambda x: x['best_val_acc'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"BEST CONFIGURATION: {best_result['config'].replace('_', ' ').title()}\")\n",
    "print(f\"  Validation Accuracy: {best_result['best_val_acc']:.2f}%\")\n",
    "print(f\"  Number of Bands: {best_result['n_bands']}\")\n",
    "print(f\"  Model saved at: {best_result['output_dir']}/best_model.pth\")\n",
    "if best_result['n_components']:\n",
    "    print(f\"  PCA model saved at: {best_result['output_dir']}/pca_model.pkl\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save comparison\n",
    "comparison_file = os.path.join(OUTPUT_DIR, 'comparison_results.json')\n",
    "with open(comparison_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'results': results,\n",
    "        'best_config': best_result,\n",
    "        'config': CONFIG\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Comparison saved to: {comparison_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation accuracy comparison\n",
    "configs = [r['config'].replace('_', ' ').title() for r in results]\n",
    "val_accs = [r['best_val_acc'] for r in results]\n",
    "bands = [r['n_bands'] for r in results]\n",
    "\n",
    "ax1.bar(configs, val_accs, alpha=0.7, color='steelblue')\n",
    "ax1.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax1.set_title('Best Validation Accuracy by Configuration', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Accuracy vs Number of Bands\n",
    "ax2.scatter(bands, val_accs, s=100, alpha=0.7, color='darkgreen')\n",
    "for i, config in enumerate(configs):\n",
    "    ax2.annotate(config, (bands[i], val_accs[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "ax2.set_xlabel('Number of Bands', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Accuracy vs Dimensionality', fontsize=14, fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'comparison_plot.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Plot saved to: {os.path.join(OUTPUT_DIR, 'comparison_plot.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Download Results to Local Machine\n",
    "\n",
    "Run this cell to download all results as a zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file of results\n",
    "import shutil\n",
    "\n",
    "zip_path = '/content/colab_results.zip'\n",
    "shutil.make_archive('/content/colab_results', 'zip', OUTPUT_DIR)\n",
    "\n",
    "print(f\"✓ Results zipped to: {zip_path}\")\n",
    "print(f\"\\nDownload the file using:\")\n",
    "print(f\"  from google.colab import files\")\n",
    "print(f\"  files.download('{zip_path}')\")\n",
    "\n",
    "# Uncomment to auto-download\n",
    "# from google.colab import files\n",
    "# files.download(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. ✓ Loads hyperspectral data from Google Drive\n",
    "2. ✓ Applies PCA for dimensionality reduction\n",
    "3. ✓ Trains pixel-wise 1D CNN classifier\n",
    "4. ✓ Compares multiple PCA configurations\n",
    "5. ✓ Saves best model and results\n",
    "\n",
    "**Next Steps:**\n",
    "- Download the best model and PCA reducer\n",
    "- Use for inference on test images\n",
    "- Fine-tune hyperparameters if needed"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
