{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast PCA Training with Band Filtering on Google Colab\n",
    "\n",
    "**New approach: 2-5x faster PCA training!**\n",
    "\n",
    "## How it works:\n",
    "1. Filter out noisy bands BEFORE PCA (removes 20% noisiest bands)\n",
    "2. Apply PCA on clean bands only (much faster!)\n",
    "3. Train 1D CNN classifier\n",
    "\n",
    "**Speed:** 459 → 367 clean bands → 120 PCA components (2-3x faster training)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions:\n",
    "1. **Enable GPU:** Runtime → Change runtime type → GPU (T4/V100/A100)\n",
    "2. **Upload dataset to Google Drive:**\n",
    "   - `MyDrive/plastic_classification/training_dataset/`\n",
    "   - `MyDrive/plastic_classification/Ground_Truth/labels.json`\n",
    "3. **Update paths** in Configuration section below\n",
    "4. **Run all cells**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scikit-learn pillow tqdm matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration (⚙️ ADJUST THESE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PATHS - UPDATE THESE TO MATCH YOUR GOOGLE DRIVE\n",
    "# ==============================================================================\n",
    "\n",
    "DRIVE_BASE = '/content/drive/MyDrive/plastic_classification'\n",
    "TRAIN_DATASET = os.path.join(DRIVE_BASE, 'training_dataset')\n",
    "LABEL_PATH = os.path.join(DRIVE_BASE, 'Ground_Truth/labels.json')\n",
    "OUTPUT_DIR = os.path.join(DRIVE_BASE, 'colab_results_fast_pca')\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# BAND FILTERING PARAMETERS - ADJUST FOR SPEED/QUALITY TRADEOFF\n",
    "# ==============================================================================\n",
    "\n",
    "BAND_FILTER_CONFIG = {\n",
    "    'keep_percentage': 80.0,        # Keep top 80% of bands by SNR\n",
    "                                    # Higher (90) = safer, slower\n",
    "                                    # Lower (70) = faster, more aggressive\n",
    "    \n",
    "    'saturation_threshold': 5.0,    # Max % saturated pixels\n",
    "    'darkness_threshold': 5.0,      # Max % dark pixels\n",
    "}\n",
    "\n",
    "# Alternative: Use manual thresholds instead of keep_percentage\n",
    "# Uncomment and set keep_percentage=None to use these:\n",
    "# BAND_FILTER_CONFIG = {\n",
    "#     'keep_percentage': None,\n",
    "#     'snr_threshold': 10.0,\n",
    "#     'variance_threshold': 0.001,\n",
    "#     'saturation_threshold': 5.0,\n",
    "#     'darkness_threshold': 5.0,\n",
    "# }\n",
    "\n",
    "# ==============================================================================\n",
    "# PCA PARAMETERS\n",
    "# ==============================================================================\n",
    "\n",
    "PCA_CONFIG = {\n",
    "    'n_components': None,           # None = auto-select from variance\n",
    "    'pca_variance_threshold': 0.99, # Keep 99% variance\n",
    "                                    # Higher (0.999) = more components\n",
    "                                    # Lower (0.95) = fewer components\n",
    "    'standardize': True,\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAINING PARAMETERS\n",
    "# ==============================================================================\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'n_classes': 11,\n",
    "    'batch_size': 1024,             # Large batch for GPU\n",
    "    'n_epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'dropout_rate': 0.5,\n",
    "    'train_split': 0.9,\n",
    "    'num_workers': 2,\n",
    "    'max_samples': None,            # None = use all samples\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# COMPARISON MODE - Test multiple keep_percentage values\n",
    "# ==============================================================================\n",
    "\n",
    "RUN_COMPARISON = False              # Set to True to compare multiple configs\n",
    "COMPARISON_CONFIGS = [90.0, 80.0, 70.0, 60.0]  # Keep percentages to test\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Band filtering: Keep {BAND_FILTER_CONFIG['keep_percentage']}% by SNR\")\n",
    "print(f\"  PCA variance: {PCA_CONFIG['pca_variance_threshold']*100}%\")\n",
    "print(f\"  Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['n_epochs']}\")\n",
    "print(f\"  Comparison mode: {RUN_COMPARISON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Band Quality Filter Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BandQualityFilter:\n",
    "    \"\"\"Filter bands based on quality metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, snr_threshold=None, variance_threshold=None,\n",
    "                 saturation_threshold=None, darkness_threshold=None,\n",
    "                 keep_percentage=None):\n",
    "        self.snr_threshold = snr_threshold\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.saturation_threshold = saturation_threshold\n",
    "        self.darkness_threshold = darkness_threshold\n",
    "        self.keep_percentage = keep_percentage\n",
    "        \n",
    "        self.band_metrics = None\n",
    "        self.good_band_indices = None\n",
    "        self.filtered_wavelengths = None\n",
    "    \n",
    "    def calculate_metrics(self, hypercube, wavelengths=None):\n",
    "        \"\"\"Calculate quality metrics for each band.\"\"\"\n",
    "        n_bands = hypercube.shape[0]\n",
    "        metrics = []\n",
    "        \n",
    "        print(f\"Calculating quality metrics for {n_bands} bands...\")\n",
    "        for i in tqdm(range(n_bands), desc=\"Analyzing bands\"):\n",
    "            band = hypercube[i]\n",
    "            \n",
    "            mean_val = np.mean(band)\n",
    "            std_val = np.std(band)\n",
    "            snr = mean_val / (std_val + 1e-8)\n",
    "            variance = np.var(band)\n",
    "            saturation_pct = (np.sum(band >= 0.98) / band.size) * 100\n",
    "            darkness_pct = (np.sum(band <= 0.02) / band.size) * 100\n",
    "            \n",
    "            metrics.append({\n",
    "                'band_idx': i,\n",
    "                'wavelength': wavelengths[i] if wavelengths else i,\n",
    "                'snr': snr,\n",
    "                'variance': variance,\n",
    "                'mean': mean_val,\n",
    "                'std': std_val,\n",
    "                'saturation_pct': saturation_pct,\n",
    "                'darkness_pct': darkness_pct\n",
    "            })\n",
    "        \n",
    "        self.band_metrics = metrics\n",
    "        return metrics\n",
    "    \n",
    "    def filter_bands(self, hypercube, wavelengths=None):\n",
    "        \"\"\"Filter bands based on quality.\"\"\"\n",
    "        if self.band_metrics is None:\n",
    "            self.calculate_metrics(hypercube, wavelengths)\n",
    "        \n",
    "        n_bands = len(self.band_metrics)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"BAND QUALITY FILTERING\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Percentile-based filtering\n",
    "        if self.keep_percentage is not None:\n",
    "            snr_values = [m['snr'] for m in self.band_metrics]\n",
    "            percentile = 100 - self.keep_percentage\n",
    "            snr_cutoff = np.percentile(snr_values, percentile)\n",
    "            \n",
    "            good_indices = [i for i, m in enumerate(self.band_metrics) \n",
    "                          if m['snr'] >= snr_cutoff]\n",
    "            \n",
    "            print(f\"Method: Keep top {self.keep_percentage}% by SNR\")\n",
    "            print(f\"SNR cutoff: {snr_cutoff:.2f}\")\n",
    "        \n",
    "        # Threshold-based filtering\n",
    "        else:\n",
    "            good_indices = list(range(n_bands))\n",
    "            print(f\"Method: Threshold-based\")\n",
    "            \n",
    "            if self.snr_threshold:\n",
    "                before = len(good_indices)\n",
    "                good_indices = [i for i in good_indices \n",
    "                              if self.band_metrics[i]['snr'] >= self.snr_threshold]\n",
    "                print(f\"  SNR ≥ {self.snr_threshold}: Removed {before - len(good_indices)} bands\")\n",
    "            \n",
    "            if self.variance_threshold:\n",
    "                before = len(good_indices)\n",
    "                good_indices = [i for i in good_indices \n",
    "                              if self.band_metrics[i]['variance'] >= self.variance_threshold]\n",
    "                print(f\"  Variance ≥ {self.variance_threshold}: Removed {before - len(good_indices)} bands\")\n",
    "            \n",
    "            if self.saturation_threshold:\n",
    "                before = len(good_indices)\n",
    "                good_indices = [i for i in good_indices \n",
    "                              if self.band_metrics[i]['saturation_pct'] <= self.saturation_threshold]\n",
    "                print(f\"  Saturation ≤ {self.saturation_threshold}%: Removed {before - len(good_indices)} bands\")\n",
    "            \n",
    "            if self.darkness_threshold:\n",
    "                before = len(good_indices)\n",
    "                good_indices = [i for i in good_indices \n",
    "                              if self.band_metrics[i]['darkness_pct'] <= self.darkness_threshold]\n",
    "                print(f\"  Darkness ≤ {self.darkness_threshold}%: Removed {before - len(good_indices)} bands\")\n",
    "        \n",
    "        self.good_band_indices = good_indices\n",
    "        filtered_hypercube = hypercube[good_indices]\n",
    "        \n",
    "        if wavelengths:\n",
    "            self.filtered_wavelengths = [wavelengths[i] for i in good_indices]\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Original: {n_bands} bands\")\n",
    "        print(f\"  Filtered: {len(good_indices)} bands\")\n",
    "        print(f\"  Removed: {n_bands - len(good_indices)} bands ({(n_bands - len(good_indices))/n_bands*100:.1f}%)\")\n",
    "        \n",
    "        return filtered_hypercube, good_indices, self.filtered_wavelengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PCA with Band Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAWithBandFiltering:\n",
    "    \"\"\"PCA with automatic band quality pre-filtering.\"\"\"\n",
    "    \n",
    "    def __init__(self, band_filter_config, pca_config):\n",
    "        self.band_filter = BandQualityFilter(**band_filter_config)\n",
    "        self.pca_config = pca_config\n",
    "        \n",
    "        self.pca = None\n",
    "        self.scaler = None\n",
    "        self.n_components_selected = None\n",
    "        self.explained_variance_ratio = None\n",
    "        \n",
    "        self.good_band_indices = None\n",
    "        self.filtered_wavelengths = None\n",
    "        self.n_original_bands = None\n",
    "        self.n_filtered_bands = None\n",
    "    \n",
    "    def fit(self, hypercube, wavelengths=None):\n",
    "        \"\"\"Fit PCA on filtered bands.\"\"\"\n",
    "        self.n_original_bands = hypercube.shape[0]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FAST PCA WITH BAND FILTERING\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Original bands: {self.n_original_bands}\")\n",
    "        \n",
    "        # Step 1: Filter noisy bands\n",
    "        filtered_hypercube, good_indices, filtered_wavelengths = self.band_filter.filter_bands(\n",
    "            hypercube, wavelengths\n",
    "        )\n",
    "        \n",
    "        self.good_band_indices = good_indices\n",
    "        self.filtered_wavelengths = filtered_wavelengths\n",
    "        self.n_filtered_bands = len(good_indices)\n",
    "        \n",
    "        # Step 2: Apply PCA\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"PCA FITTING ON CLEAN BANDS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        n_bands, height, width = filtered_hypercube.shape\n",
    "        X = filtered_hypercube.reshape(n_bands, -1).T\n",
    "        \n",
    "        if self.pca_config['standardize']:\n",
    "            self.scaler = StandardScaler()\n",
    "            X = self.scaler.fit_transform(X)\n",
    "            print(f\"✓ Data standardized\")\n",
    "        \n",
    "        # Determine components\n",
    "        if self.pca_config['n_components'] is None:\n",
    "            pca_full = PCA()\n",
    "            pca_full.fit(X)\n",
    "            cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "            self.n_components_selected = np.argmax(\n",
    "                cumsum_var >= self.pca_config['pca_variance_threshold']\n",
    "            ) + 1\n",
    "            print(f\"✓ Auto-selected {self.n_components_selected} components \"\n",
    "                  f\"({self.pca_config['pca_variance_threshold']*100:.0f}% variance)\")\n",
    "        else:\n",
    "            self.n_components_selected = self.pca_config['n_components']\n",
    "            print(f\"✓ Using {self.n_components_selected} components\")\n",
    "        \n",
    "        # Fit PCA\n",
    "        self.pca = PCA(n_components=self.n_components_selected)\n",
    "        self.pca.fit(X)\n",
    "        self.explained_variance_ratio = self.pca.explained_variance_ratio_\n",
    "        \n",
    "        total_var = np.sum(self.explained_variance_ratio)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FINAL RESULTS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Dimensionality reduction:\")\n",
    "        print(f\"  {self.n_original_bands} → {self.n_filtered_bands} (filtered) → {self.n_components_selected} (PCA)\")\n",
    "        print(f\"  Total reduction: {(1 - self.n_components_selected/self.n_original_bands)*100:.1f}%\")\n",
    "        print(f\"  PCA variance: {total_var*100:.2f}%\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, spectrum):\n",
    "        \"\"\"Transform full spectrum to PCA space.\"\"\"\n",
    "        # Filter to good bands\n",
    "        spectrum_filtered = spectrum[self.good_band_indices]\n",
    "        \n",
    "        # Standardize\n",
    "        spectrum_2d = spectrum_filtered.reshape(1, -1)\n",
    "        if self.pca_config['standardize'] and self.scaler:\n",
    "            spectrum_2d = self.scaler.transform(spectrum_2d)\n",
    "        \n",
    "        # PCA\n",
    "        spectrum_pca = self.pca.transform(spectrum_2d)\n",
    "        return spectrum_pca.flatten()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model.\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'band_filter': self.band_filter,\n",
    "                'pca': self.pca,\n",
    "                'scaler': self.scaler,\n",
    "                'n_components_selected': self.n_components_selected,\n",
    "                'good_band_indices': self.good_band_indices,\n",
    "                'filtered_wavelengths': self.filtered_wavelengths,\n",
    "                'n_original_bands': self.n_original_bands,\n",
    "                'n_filtered_bands': self.n_filtered_bands,\n",
    "            }, f)\n",
    "        print(f\"✓ Model saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Hyperspectral Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hypercube(dataset_path):\n",
    "    \"\"\"Load hypercube from dataset.\"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    with open(dataset_path / 'header.json', 'r') as f:\n",
    "        header = json.load(f)\n",
    "    wavelengths = header['wavelength (nm)']\n",
    "    \n",
    "    print(f\"Loading {len(wavelengths)} bands...\")\n",
    "    bands = []\n",
    "    for i in tqdm(range(1, len(wavelengths) + 1), desc='Loading'):\n",
    "        img_path = dataset_path / f'ImagesStack{i:03d}.png'\n",
    "        if img_path.exists():\n",
    "            img = np.array(Image.open(img_path).convert('L'), dtype=np.float32) / 255.0\n",
    "            bands.append(img)\n",
    "    \n",
    "    hypercube = np.stack(bands, axis=0)\n",
    "    print(f\"✓ Loaded: {hypercube.shape}\")\n",
    "    \n",
    "    return hypercube, wavelengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastPCADataset(Dataset):\n",
    "    \"\"\"Dataset with band filtering + PCA.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, label_path, pca_selector=None, max_samples=None):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.pca_selector = pca_selector\n",
    "        \n",
    "        with open(self.dataset_path / 'header.json', 'r') as f:\n",
    "            header = json.load(f)\n",
    "        self.wavelengths = header['wavelength (nm)']\n",
    "        self.n_bands = len(self.wavelengths)\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            labels_data = json.load(f)\n",
    "        \n",
    "        self.samples = []\n",
    "        for label_info in labels_data:\n",
    "            class_id = label_info['label']\n",
    "            for coord in label_info['coordinates']:\n",
    "                x, y = coord\n",
    "                self.samples.append({'x': x, 'y': y, 'label': class_id})\n",
    "        \n",
    "        if max_samples:\n",
    "            self.samples = self.samples[:max_samples]\n",
    "        \n",
    "        print(f\"✓ Dataset: {len(self.samples):,} samples\")\n",
    "    \n",
    "    def load_spectrum(self, x, y):\n",
    "        spectrum = np.zeros(self.n_bands, dtype=np.float32)\n",
    "        for i in range(1, self.n_bands + 1):\n",
    "            img_path = self.dataset_path / f'ImagesStack{i:03d}.png'\n",
    "            if img_path.exists():\n",
    "                img = Image.open(img_path).convert('L')\n",
    "                spectrum[i - 1] = np.array(img)[y, x] / 255.0\n",
    "        return spectrum\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        x, y, label = sample['x'], sample['y'], sample['label']\n",
    "        \n",
    "        spectrum = self.load_spectrum(x, y)\n",
    "        \n",
    "        if self.pca_selector:\n",
    "            spectrum = self.pca_selector.transform(spectrum)\n",
    "        \n",
    "        return torch.from_numpy(spectrum).float(), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralCNN1D(nn.Module):\n",
    "    \"\"\"1D CNN for spectral classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_bands, n_classes, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.flat_size = 256 * (n_bands // 4)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flat_size, 512)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(256, n_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, n_epochs, lr, output_dir):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=10, T_mult=2, eta_min=lr/10\n",
    "    )\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for spectra, labels in tqdm(train_loader, desc=f'Epoch {epoch}/{n_epochs} [Train]'):\n",
    "            spectra, labels = spectra.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(spectra)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for spectra, labels in tqdm(val_loader, desc=f'Epoch {epoch}/{n_epochs} [Val]'):\n",
    "                spectra, labels = spectra.to(device), labels.to(device)\n",
    "                outputs = model(spectra)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Acc={train_acc:.2f}% | \"\n",
    "              f\"Val Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, os.path.join(output_dir, 'best_model.pth'))\n",
    "            print(f\"✓ Best model saved (Val Acc: {val_acc:.2f}%)\")\n",
    "    \n",
    "    return history, best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Load Hypercube for PCA Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOADING HYPERCUBE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "hypercube, wavelengths = load_hypercube(TRAIN_DATASET)\n",
    "print(f\"\\n✓ Shape: {hypercube.shape}\")\n",
    "print(f\"✓ Wavelengths: {wavelengths[0]:.1f} - {wavelengths[-1]:.1f} nm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_COMPARISON:\n",
    "    # Compare multiple configurations\n",
    "    results = []\n",
    "    \n",
    "    for keep_pct in COMPARISON_CONFIGS:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TESTING: Keep {keep_pct}%\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Update config\n",
    "        band_config = BAND_FILTER_CONFIG.copy()\n",
    "        band_config['keep_percentage'] = keep_pct\n",
    "        \n",
    "        # Fit PCA\n",
    "        pca_selector = PCAWithBandFiltering(band_config, PCA_CONFIG)\n",
    "        pca_selector.fit(hypercube, wavelengths)\n",
    "        \n",
    "        # Save PCA\n",
    "        exp_dir = os.path.join(OUTPUT_DIR, f'keep_{int(keep_pct)}')\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "        pca_selector.save(os.path.join(exp_dir, 'pca_model.pkl'))\n",
    "        \n",
    "        # Create dataset\n",
    "        full_dataset = FastPCADataset(TRAIN_DATASET, LABEL_PATH, pca_selector,\n",
    "                                     TRAINING_CONFIG['max_samples'])\n",
    "        \n",
    "        n_samples = len(full_dataset)\n",
    "        n_train = int(n_samples * TRAINING_CONFIG['train_split'])\n",
    "        n_val = n_samples - n_train\n",
    "        \n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [n_train, n_val])\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=TRAINING_CONFIG['batch_size'],\n",
    "                                 shuffle=True, num_workers=TRAINING_CONFIG['num_workers'], pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=TRAINING_CONFIG['batch_size'],\n",
    "                               shuffle=False, num_workers=TRAINING_CONFIG['num_workers'], pin_memory=True)\n",
    "        \n",
    "        # Create model\n",
    "        model = SpectralCNN1D(pca_selector.n_components_selected,\n",
    "                             TRAINING_CONFIG['n_classes'],\n",
    "                             TRAINING_CONFIG['dropout_rate']).to(device)\n",
    "        \n",
    "        # Train\n",
    "        history, best_acc = train_model(model, train_loader, val_loader,\n",
    "                                       TRAINING_CONFIG['n_epochs'],\n",
    "                                       TRAINING_CONFIG['learning_rate'],\n",
    "                                       exp_dir)\n",
    "        \n",
    "        results.append({\n",
    "            'keep_percentage': keep_pct,\n",
    "            'original_bands': pca_selector.n_original_bands,\n",
    "            'filtered_bands': pca_selector.n_filtered_bands,\n",
    "            'pca_components': pca_selector.n_components_selected,\n",
    "            'best_val_acc': best_acc,\n",
    "            'reduction': (1 - pca_selector.n_components_selected/pca_selector.n_original_bands) * 100\n",
    "        })\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"COMPARISON SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n{'Keep %':<10} {'Bands Reduction':<30} {'PCA':<8} {'Total Red.':<12} {'Val Acc':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for r in results:\n",
    "        bands_str = f\"{r['original_bands']}→{r['filtered_bands']}→{r['pca_components']}\"\n",
    "        print(f\"{r['keep_percentage']:<10.0f} {bands_str:<30} {r['pca_components']:<8} \"\n",
    "              f\"{r['reduction']:<11.1f}% {r['best_val_acc']:.2f}%\")\n",
    "    \n",
    "    # Best\n",
    "    best = max(results, key=lambda x: x['best_val_acc'])\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BEST: Keep {best['keep_percentage']}%\")\n",
    "    print(f\"  Accuracy: {best['best_val_acc']:.2f}%\")\n",
    "    print(f\"  Reduction: {best['reduction']:.1f}%\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "else:\n",
    "    # Single configuration\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING WITH SINGLE CONFIGURATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Fit PCA\n",
    "    pca_selector = PCAWithBandFiltering(BAND_FILTER_CONFIG, PCA_CONFIG)\n",
    "    pca_selector.fit(hypercube, wavelengths)\n",
    "    \n",
    "    # Save PCA\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    pca_selector.save(os.path.join(OUTPUT_DIR, 'pca_model.pkl'))\n",
    "    \n",
    "    # Create dataset\n",
    "    full_dataset = FastPCADataset(TRAIN_DATASET, LABEL_PATH, pca_selector,\n",
    "                                 TRAINING_CONFIG['max_samples'])\n",
    "    \n",
    "    n_samples = len(full_dataset)\n",
    "    n_train = int(n_samples * TRAINING_CONFIG['train_split'])\n",
    "    n_val = n_samples - n_train\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [n_train, n_val])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=TRAINING_CONFIG['batch_size'],\n",
    "                             shuffle=True, num_workers=TRAINING_CONFIG['num_workers'], pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=TRAINING_CONFIG['batch_size'],\n",
    "                           shuffle=False, num_workers=TRAINING_CONFIG['num_workers'], pin_memory=True)\n",
    "    \n",
    "    # Create model\n",
    "    model = SpectralCNN1D(pca_selector.n_components_selected,\n",
    "                         TRAINING_CONFIG['n_classes'],\n",
    "                         TRAINING_CONFIG['dropout_rate']).to(device)\n",
    "    \n",
    "    # Train\n",
    "    history, best_acc = train_model(model, train_loader, val_loader,\n",
    "                                   TRAINING_CONFIG['n_epochs'],\n",
    "                                   TRAINING_CONFIG['learning_rate'],\n",
    "                                   OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\n✓ Training complete!\")\n",
    "    print(f\"  Best accuracy: {best_acc:.2f}%\")\n",
    "    print(f\"  Model: {OUTPUT_DIR}/best_model.pth\")\n",
    "    print(f\"  PCA: {OUTPUT_DIR}/pca_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip for download\n",
    "import shutil\n",
    "\n",
    "zip_path = '/content/fast_pca_results.zip'\n",
    "shutil.make_archive('/content/fast_pca_results', 'zip', OUTPUT_DIR)\n",
    "\n",
    "print(f\"✓ Results zipped: {zip_path}\")\n",
    "print(f\"\\nTo download:\")\n",
    "print(f\"  from google.colab import files\")\n",
    "print(f\"  files.download('{zip_path}')\")\n",
    "\n",
    "# Uncomment to auto-download\n",
    "# from google.colab import files\n",
    "# files.download(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**This notebook implements fast PCA training with band quality pre-filtering:**\n",
    "\n",
    "✅ **Filters noisy bands** before PCA (2-5x faster PCA fitting)\n",
    "\n",
    "✅ **Easy parameter adjustment** in Configuration section\n",
    "\n",
    "✅ **Comparison mode** to find best keep_percentage\n",
    "\n",
    "✅ **Saves both models** (PCA + classifier)\n",
    "\n",
    "**Key Parameters to Adjust:**\n",
    "- `keep_percentage`: 60-90% (higher = safer, lower = faster)\n",
    "- `pca_variance_threshold`: 0.95-0.999 (higher = more info)\n",
    "- `RUN_COMPARISON`: True to test multiple configs\n",
    "\n",
    "**Results saved in Google Drive:**\n",
    "- `colab_results_fast_pca/best_model.pth`\n",
    "- `colab_results_fast_pca/pca_model.pkl`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
